{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Trainer\n",
        "Train the models on specific datasets, using feature extractor or finetuning\n",
        "\n",
        "**Author**\n",
        "\n",
        "`Nathan Inkawhich <https://github.com/inkawhich>`\n",
        "\n",
        "**Customizations**\n",
        "\n",
        "`Marco Alecci <https://github.com/MarcoAlecci>`\n",
        "\n",
        "`Francesco Marchiori <https://github.com/FrancescoMarchiori>`\n",
        "\n",
        "`Luca Martinelli <https://github.com/luca-martinelli-09>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luca-martinelli-09/orco-gan/blob/main/modelTrainer.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "gNWDiOw2FJ3B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# @markdown ## Setup project\n",
        "# @markdown This section will download the datasets from GitHub to use for the training phase\n",
        "\n",
        "if not os.path.exists(\"./datasets\"):\n",
        "    !git clone \"https://github.com/luca-martinelli-09/orco-gan.git\"\n",
        "\n",
        "    %cd orco-gan/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDT0zJ4_ZZPd",
        "outputId": "8979b9cd-17a3-427b-8639-758b70f53c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 1.10.1\n",
            "Torchvision Version: 0.11.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\franc\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\franc\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"Torchvision Version:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Detect if we have a GPU available\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set a manual seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 151836\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVyUpOe1ZZPe"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "tB8MBKs-ZZPf"
      },
      "outputs": [],
      "source": [
        "# @markdown ## Dataset\n",
        "# @markdown Directory of the dataset\n",
        "dataset_dir = \"bing\" # @param [\"bing\", \"ddg\", \"google\"]\n",
        "data_dir = r\"./datasets/{}\".format(dataset_dir)\n",
        "\n",
        "# @markdown Number of classes in the dataset\n",
        "num_classes = 2 # @param {type:\"integer\", min: 1}\n",
        "\n",
        "# @markdown Number of maximum samples per class (training)\n",
        "num_samples = 3500  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown When True execute training for every ratio\n",
        "all_ratios = True  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Ratio between classes cat and dog\n",
        "ratios = \"50,50\" # @param [\"50,50\", \"40,60\", \"30,70\", \"20,80\"]\n",
        "ratio = int(ratios.split(\",\")[0]) / int(ratios.split(\",\")[1])\n",
        "dataset_sizes = [math.ceil(num_samples * ratio), num_samples]\n",
        "\n",
        "if all_ratios:\n",
        "    possible_ratios = [\"50,50\", \"40,60\", \"30,70\", \"20,80\"]\n",
        "    all_dataset_sizes = []\n",
        "    for ratios in possible_ratios:\n",
        "        ratio = int(ratios.split(\",\")[0]) / int(ratios.split(\",\")[1])\n",
        "        dataset_sizes = [math.ceil(num_samples * ratio), num_samples]\n",
        "        all_dataset_sizes.append(dataset_sizes)\n",
        "\n",
        "# @markdown Check images in the dataset before training\n",
        "check_images = False # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Save PIL loaded image in a dictionary (consume more memory)\n",
        "use_cache = True  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown ## DataLoader\n",
        "\n",
        "num_workers = 0 # @param {type:\"integer\", min: 1}\n",
        "\n",
        "pin_memory = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ## Model\n",
        "\n",
        "# @markdown Model to use\n",
        "model_name = \"resnet\" # @param [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\", \"inception\"]\n",
        "\n",
        "# @markdown Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 16 # @param {type:\"integer\", min: 1}\n",
        "\n",
        "# @markdown Number of epochs to train for\n",
        "num_epochs = 500 # @param {type:\"integer\", min: 1}\n",
        "\n",
        "# @markdown Patience for early stopping\n",
        "patience_es = 20 # @param {type:\"integer\", min: 1}\n",
        "\n",
        "# @markdown Delta for early stopping\n",
        "delta_es = 0.0001 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown Flag for feature extracting. When False, we finetune the whole model, \n",
        "# @markdown when True we only update the reshaped layer params\n",
        "feature_extract = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown The learning rate of the optimizer\n",
        "learning_rate = 0.001 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown The momentum of the optimizer\n",
        "momentum = 0.9 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown ## Output\n",
        "# @markdown Save the model after been trained\n",
        "save_model = False # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Save entire model (not only weights)\n",
        "save_entire_model = False # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Save all (model, history, optimizer, criterion, best_epoch)\n",
        "save_all = True # @param {type: \"boolean\"}\n",
        "\n",
        "model_save_path = \"{}_{}\".format(model_name, \"_\".join(ratios.split(\",\")))\n",
        "\n",
        "\n",
        "# Normalization values\n",
        "normalization_vals = {\n",
        "    \"bing\": {\n",
        "        \"train\": [[0.5407, 0.5059, 0.4523], [0.2830, 0.2794, 0.2898]],\n",
        "        \"val\": [[0.5341, 0.5012, 0.4385], [0.2809, 0.2752, 0.2863]],\n",
        "        \"test\": [[0.5257, 0.4953, 0.4290], [0.2799, 0.2730, 0.2844]]\n",
        "    },\n",
        "    \"ddg\": {\n",
        "        \"train\": [[0.5366, 0.5061, 0.4544], [0.2860, 0.2820, 0.2917]],\n",
        "        \"val\": [[0.5364, 0.5036, 0.4522], [0.2868, 0.2817, 0.2917]],\n",
        "        \"test\": [[0.5323, 0.5006, 0.4465], [0.2825, 0.2784, 0.2881]]\n",
        "    },\n",
        "    \"google\": {\n",
        "        \"train\": [[0.5635, 0.5371, 0.4781], [0.2899, 0.2861, 0.3035]],\n",
        "        \"val\": [[0.5653, 0.5397, 0.4751], [0.2872, 0.2835, 0.3018]],\n",
        "        \"test\": [[0.5736, 0.5468, 0.4893], [0.2954, 0.2914, 0.3083]]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2rD69LjZZPg"
      },
      "source": [
        "Helper Functions\n",
        "----------------\n",
        "\n",
        "Before we write the code for adjusting the models, lets define a few\n",
        "helper functions.\n",
        "\n",
        "### Model Training and Validation Code\n",
        "\n",
        "The ``train_model`` function handles the training and validation of a\n",
        "given model. As input, it takes a PyTorch model, a dictionary of\n",
        "dataloaders, a loss function, an optimizer, a specified number of epochs\n",
        "to train and validate for, and a boolean flag for when the model is an\n",
        "Inception model. The *is_inception* flag is used to accomodate the\n",
        "*Inception v3* model, as that architecture uses an auxiliary output and\n",
        "the overall model loss respects both the auxiliary output and the final\n",
        "output, as described\n",
        "`here <https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958>`.\n",
        "The function trains for the specified number of epochs and after each\n",
        "epoch runs a full validation step. It also keeps track of the best\n",
        "performing model (in terms of validation accuracy), and at the end of\n",
        "training returns the best performing model. After each epoch, the\n",
        "training and validation accuracies are printed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "li-oD5wEiNYI"
      },
      "outputs": [],
      "source": [
        "def print_gpu_stats():\n",
        "    print('Using device:', device)\n",
        "    print()\n",
        "\n",
        "    # Additional Info when using cuda\n",
        "    if device.type == 'cuda':\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "        print('[ğŸ’» MEMORY USAGE]')\n",
        "        print('[ğŸ“Œ ALLOCATED]', round(\n",
        "            torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
        "        print('[ğŸ§® CACHED]', round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_scores(labels, predicted):\n",
        "    acc = torch.sum(predicted == labels) / len(predicted)\n",
        "\n",
        "    tp = (labels * predicted).sum()\n",
        "    tn = ((1 - labels) * (1 - predicted)).sum()\n",
        "    fp = ((1 - labels) * predicted).sum()\n",
        "    fn = (labels * (1 - predicted)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return acc, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CaH3YZrxZZPg"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, delta=0, patience=10):\n",
        "    since = time.time()\n",
        "    last_since = time.time()\n",
        "\n",
        "    scores_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    best_score = None\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('[ğŸ’ª EPOCH] {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        epoch_score = None\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            labels_outputs = torch.tensor([]).to(device, non_blocking=True)\n",
        "            labels_targets = torch.tensor([]).to(device, non_blocking=True)\n",
        "\n",
        "            # Iterate over data\n",
        "            set_seed(SEED)\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4 * loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                labels_outputs = torch.cat([labels_outputs, preds], dim=0)\n",
        "                labels_targets = torch.cat([labels_targets, labels], dim=0)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc, epoch_prec, epoch_rec, epoch_f1 = get_scores(labels_targets, labels_outputs)\n",
        "\n",
        "            print('[ğŸ—ƒï¸ {}] Loss: {:.4f} Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F-Score: {:.4f}'.format(\n",
        "                phase.upper(), epoch_loss, epoch_acc, epoch_prec, epoch_rec, epoch_f1))\n",
        "            \n",
        "            time_elapsed = time.time() - last_since\n",
        "            last_since = time.time()\n",
        "            print(\"\\t[ğŸ•‘] {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
        "            \n",
        "            if phase == 'val':\n",
        "                epoch_score = epoch_f1\n",
        "\n",
        "                # deep copy the model\n",
        "                if epoch_f1 > best_f1:\n",
        "                    best_f1 = epoch_f1\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                \n",
        "                # Store scores history\n",
        "                scores_history.append({\n",
        "                    \"loss\": epoch_loss,\n",
        "                    \"acc\": epoch_acc.cpu().numpy(),\n",
        "                    \"precision\": epoch_prec.cpu().numpy(),\n",
        "                    \"recall\": epoch_rec.cpu().numpy(),\n",
        "                    \"f1\": epoch_f1.cpu().numpy()\n",
        "                })\n",
        "        \n",
        "        if best_score is None:\n",
        "            best_score = epoch_score\n",
        "        elif epoch_score <= best_score + delta:\n",
        "            counter += 1\n",
        "            print(\"\\t[âš ï¸ EARLY STOPPING] {}/{}\".format(counter, patience))\n",
        "            if counter >= patience:\n",
        "                break\n",
        "        else:\n",
        "            best_score = epoch_score\n",
        "            counter = 0\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print()\n",
        "    print('[ğŸ•‘ TRAINING COMPLETE] {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('[ğŸ¥‡ BEST SCORE] F-Score: {:4f}'.format(best_f1))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, scores_history, best_score, best_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj3RaFh-ZZPh"
      },
      "source": [
        "### Set Model Parametersâ€™ .requires_grad attribute\n",
        "\n",
        "This helper function sets the ``.requires_grad`` attribute of the\n",
        "parameters in the model to False when we are feature extracting. By\n",
        "default, when we load a pretrained model all of the parameters have\n",
        "``.requires_grad=True``, which is fine if we are training from scratch\n",
        "or finetuning. However, if we are feature extracting and only want to\n",
        "compute gradients for the newly initialized layer then we want all of\n",
        "the other parameters to not require gradients. This will make more sense\n",
        "later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bNQHdOIXZZPi"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep9f6Jr_ZZPi"
      },
      "source": [
        "## Initialize and Reshape the Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4yCwPSTZZPl",
        "outputId": "0aeb49a0-1cd7-4be8-f915-ef95aec1ab50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfH8SXR7ZZPn"
      },
      "source": [
        "Load Data\n",
        "---------\n",
        "\n",
        "Now that we know what the input size must be, we can initialize the data\n",
        "transforms, image datasets, and the dataloaders. Notice, the models were\n",
        "pretrained with the hard-coded normalization values, as described\n",
        "`here <https://pytorch.org/docs/master/torchvision/models.html>`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEkr8kJPZZPn",
        "outputId": "b1540b97-3ad2-4b6d-db58-1381a8f0441d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[ğŸ—ƒï¸ TRAIN]\n",
            "[ğŸ§® # ELEMENTS] cat: 3500\n",
            "[ğŸ§® # ELEMENTS] dog: 3500\n",
            "\n",
            "[ğŸ—ƒï¸ VAL]\n",
            "[ğŸ§® # ELEMENTS] cat: 500\n",
            "[ğŸ§® # ELEMENTS] dog: 500\n",
            "\n",
            "[ğŸ—ƒï¸ TRAIN]\n",
            "[ğŸ§® # ELEMENTS] cat: 2334\n",
            "[ğŸ§® # ELEMENTS] dog: 3500\n",
            "\n",
            "[ğŸ—ƒï¸ VAL]\n",
            "[ğŸ§® # ELEMENTS] cat: 500\n",
            "[ğŸ§® # ELEMENTS] dog: 500\n",
            "\n",
            "[ğŸ—ƒï¸ TRAIN]\n",
            "[ğŸ§® # ELEMENTS] cat: 1500\n",
            "[ğŸ§® # ELEMENTS] dog: 3500\n",
            "\n",
            "[ğŸ—ƒï¸ VAL]\n",
            "[ğŸ§® # ELEMENTS] cat: 500\n",
            "[ğŸ§® # ELEMENTS] dog: 500\n",
            "\n",
            "[ğŸ—ƒï¸ TRAIN]\n",
            "[ğŸ§® # ELEMENTS] cat: 875\n",
            "[ğŸ§® # ELEMENTS] dog: 3500\n",
            "\n",
            "[ğŸ—ƒï¸ VAL]\n",
            "[ğŸ§® # ELEMENTS] cat: 500\n",
            "[ğŸ§® # ELEMENTS] dog: 500\n"
          ]
        }
      ],
      "source": [
        "from imageLimitedDataset import ImageLimitedDataset\n",
        "\n",
        "# Data resize and normalization\n",
        "normalization_pars = normalization_vals[dataset_dir]\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            normalization_pars[\"train\"][0],\n",
        "            normalization_pars[\"train\"][1]\n",
        "        )\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            normalization_pars[\"val\"][0],\n",
        "            normalization_pars[\"val\"][1]\n",
        "        )\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create training and validation datasets\n",
        "\n",
        "# Create the slices to decide which samples keep\n",
        "if not all_ratios:\n",
        "    slices = {\n",
        "        \"train\": [slice(None, cut_point) for cut_point in dataset_sizes],\n",
        "        \"val\": None,\n",
        "    }\n",
        "else:\n",
        "    all_slices = []\n",
        "    for dataset_sizes in all_dataset_sizes:\n",
        "        slices = {\n",
        "            \"train\": [slice(None, cut_point) for cut_point in dataset_sizes],\n",
        "            \"val\": None,\n",
        "        }\n",
        "        all_slices.append(slices)\n",
        "\n",
        "# Create training and validation datasets\n",
        "if not all_ratios:\n",
        "    image_datasets = {x: ImageLimitedDataset(os.path.join(data_dir, x),\n",
        "                        transform=data_transforms[x],\n",
        "                        slices=slices[x],\n",
        "                        check_images=check_images,\n",
        "                        use_cache=use_cache) for x in [\"train\", \"val\"]}\n",
        "else:\n",
        "    all_image_datasets = []\n",
        "    for slices in all_slices:\n",
        "        image_datasets = {x: ImageLimitedDataset(os.path.join(data_dir, x),\n",
        "                            transform=data_transforms[x],\n",
        "                            slices=slices[x],\n",
        "                            check_images=check_images,\n",
        "                            use_cache=use_cache) for x in [\"train\", \"val\"]}\n",
        "        all_image_datasets.append(image_datasets)\n",
        "\n",
        "# Check the sizes of the created datasets\n",
        "if not all_ratios:\n",
        "    for x in [\"train\", \"val\"]:\n",
        "        print()\n",
        "\n",
        "        print(\"[ğŸ—ƒï¸ {}]\".format(x.upper()))\n",
        "        for cls in image_datasets[x].classes:\n",
        "            cls_index = image_datasets[x].class_to_idx[cls]\n",
        "            num_cls = np.count_nonzero(np.array(image_datasets[x].targets) == cls_index)\n",
        "            print(\"[ğŸ§® # ELEMENTS] {}: {}\".format(cls, num_cls))\n",
        "\n",
        "    # Create training and validation dataloaders\n",
        "    set_seed(SEED)\n",
        "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory) for x in [\"train\", \"val\"]}\n",
        "else:\n",
        "    all_dataloaders_dict = []\n",
        "    for image_datasets in all_image_datasets:\n",
        "        for x in [\"train\", \"val\"]:\n",
        "            print()\n",
        "            \n",
        "            print(\"[ğŸ—ƒï¸ {}]\".format(x.upper()))\n",
        "            for cls in image_datasets[x].classes:\n",
        "                cls_index = image_datasets[x].class_to_idx[cls]\n",
        "                num_cls = np.count_nonzero(np.array(image_datasets[x].targets) == cls_index)\n",
        "                print(\"[ğŸ§® # ELEMENTS] {}: {}\".format(cls, num_cls))\n",
        "\n",
        "        # Create training and validation dataloaders\n",
        "        set_seed(SEED)\n",
        "        dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory) for x in [\"train\", \"val\"]}\n",
        "        all_dataloaders_dict.append(dataloaders_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIKcyosQZZPo"
      },
      "source": [
        "Create the Optimizer\n",
        "--------------------\n",
        "\n",
        "Now that the model structure is correct, the final step for finetuning\n",
        "and feature extracting is to create an optimizer that only updates the\n",
        "desired parameters. Recall that after loading the pretrained model, but\n",
        "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
        "parameterâ€™s ``.requires_grad`` attributes to False. Then the\n",
        "reinitialized layerâ€™s parameters have ``.requires_grad=True`` by\n",
        "default. So now we know that *all parameters that have\n",
        ".requires_grad=True should be optimized.* Next, we make a list of such\n",
        "parameters and input this list to the SGD algorithm constructor.\n",
        "\n",
        "To verify this, check out the printed parameters to learn. When\n",
        "finetuning, this list should be long and include all of the model\n",
        "parameters. However, when feature extracting this list should be short\n",
        "and only include the weights and biases of the reshaped layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQwRLJqqZZPo",
        "outputId": "12c079b8-f8e2-409f-cd2e-1e245781a90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ğŸ§  PARAMS TO LEARN]\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device, non_blocking=True)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are \n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"[ğŸ§  PARAMS TO LEARN]\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name, param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\", name)\n",
        "else:\n",
        "    for name, param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\", name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=learning_rate, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaAipjTqZZPo"
      },
      "source": [
        "Run Training and Validation Step\n",
        "--------------------------------\n",
        "\n",
        "Finally, the last step is to setup the loss for the model, then run the\n",
        "training and validation function for the set number of epochs. Notice,\n",
        "depending on the number of epochs this step may take a while on a CPU.\n",
        "Also, the default learning rate is not optimal for all of the models, so\n",
        "to achieve maximum accuracy it would be necessary to tune for each model\n",
        "separately.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NGF1crxZZPp",
        "outputId": "ccfc1fa6-dddc-48ad-9e0c-b85190c4e298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING WITH 50,50 DATASET RATIO\n",
            "[ğŸ’ª EPOCH] 1/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.1726 Acc: 0.9251 Pre: 0.9254 Rec: 0.9249 F-Score: 0.9251\n",
            "\t[ğŸ•‘] 0m 26s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.1188 Acc: 0.9520 Pre: 0.9154 Rec: 0.9960 F-Score: 0.9540\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "\n",
            "[ğŸ’ª EPOCH] 2/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.1119 Acc: 0.9566 Pre: 0.9568 Rec: 0.9563 F-Score: 0.9566\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0869 Acc: 0.9670 Pre: 0.9431 Rec: 0.9940 F-Score: 0.9679\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 3/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.1020 Acc: 0.9594 Pre: 0.9600 Rec: 0.9589 F-Score: 0.9594\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0728 Acc: 0.9740 Pre: 0.9558 Rec: 0.9940 F-Score: 0.9745\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 4/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0960 Acc: 0.9617 Pre: 0.9628 Rec: 0.9606 F-Score: 0.9617\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0656 Acc: 0.9780 Pre: 0.9632 Rec: 0.9940 F-Score: 0.9783\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 5/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0918 Acc: 0.9640 Pre: 0.9656 Rec: 0.9623 F-Score: 0.9639\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0616 Acc: 0.9810 Pre: 0.9688 Rec: 0.9940 F-Score: 0.9812\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 6/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0886 Acc: 0.9660 Pre: 0.9679 Rec: 0.9640 F-Score: 0.9659\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0592 Acc: 0.9820 Pre: 0.9725 Rec: 0.9920 F-Score: 0.9822\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 7/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0861 Acc: 0.9673 Pre: 0.9682 Rec: 0.9663 F-Score: 0.9673\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0576 Acc: 0.9800 Pre: 0.9724 Rec: 0.9880 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 1/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 8/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0839 Acc: 0.9680 Pre: 0.9683 Rec: 0.9677 F-Score: 0.9680\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0567 Acc: 0.9800 Pre: 0.9724 Rec: 0.9880 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 2/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 9/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0821 Acc: 0.9687 Pre: 0.9691 Rec: 0.9683 F-Score: 0.9687\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0561 Acc: 0.9790 Pre: 0.9724 Rec: 0.9860 F-Score: 0.9791\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 3/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 10/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0806 Acc: 0.9697 Pre: 0.9705 Rec: 0.9689 F-Score: 0.9697\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0557 Acc: 0.9780 Pre: 0.9723 Rec: 0.9840 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 4/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 11/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0792 Acc: 0.9699 Pre: 0.9705 Rec: 0.9691 F-Score: 0.9698\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0555 Acc: 0.9780 Pre: 0.9723 Rec: 0.9840 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 5/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 12/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0780 Acc: 0.9704 Pre: 0.9711 Rec: 0.9697 F-Score: 0.9704\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0554 Acc: 0.9760 Pre: 0.9704 Rec: 0.9820 F-Score: 0.9761\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 6/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 13/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0769 Acc: 0.9709 Pre: 0.9717 Rec: 0.9700 F-Score: 0.9708\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0554 Acc: 0.9760 Pre: 0.9704 Rec: 0.9820 F-Score: 0.9761\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 7/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 14/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0759 Acc: 0.9711 Pre: 0.9720 Rec: 0.9703 F-Score: 0.9711\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0554 Acc: 0.9760 Pre: 0.9704 Rec: 0.9820 F-Score: 0.9761\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 8/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 15/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0749 Acc: 0.9716 Pre: 0.9728 Rec: 0.9703 F-Score: 0.9715\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0554 Acc: 0.9760 Pre: 0.9704 Rec: 0.9820 F-Score: 0.9761\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 9/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 16/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0741 Acc: 0.9717 Pre: 0.9733 Rec: 0.9700 F-Score: 0.9717\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0555 Acc: 0.9760 Pre: 0.9704 Rec: 0.9820 F-Score: 0.9761\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 10/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 17/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0733 Acc: 0.9717 Pre: 0.9733 Rec: 0.9700 F-Score: 0.9717\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0557 Acc: 0.9770 Pre: 0.9723 Rec: 0.9820 F-Score: 0.9771\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 11/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 18/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0726 Acc: 0.9719 Pre: 0.9733 Rec: 0.9703 F-Score: 0.9718\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0558 Acc: 0.9780 Pre: 0.9742 Rec: 0.9820 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 12/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 19/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0719 Acc: 0.9723 Pre: 0.9736 Rec: 0.9709 F-Score: 0.9722\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0559 Acc: 0.9780 Pre: 0.9742 Rec: 0.9820 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 13/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 20/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0712 Acc: 0.9724 Pre: 0.9739 Rec: 0.9709 F-Score: 0.9724\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0561 Acc: 0.9780 Pre: 0.9742 Rec: 0.9820 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 14/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 21/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0706 Acc: 0.9724 Pre: 0.9739 Rec: 0.9709 F-Score: 0.9724\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0562 Acc: 0.9780 Pre: 0.9742 Rec: 0.9820 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 15/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 22/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0700 Acc: 0.9726 Pre: 0.9739 Rec: 0.9711 F-Score: 0.9725\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0564 Acc: 0.9780 Pre: 0.9742 Rec: 0.9820 F-Score: 0.9781\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 16/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 23/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0695 Acc: 0.9731 Pre: 0.9745 Rec: 0.9717 F-Score: 0.9731\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0566 Acc: 0.9790 Pre: 0.9761 Rec: 0.9820 F-Score: 0.9791\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 17/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 24/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0690 Acc: 0.9733 Pre: 0.9745 Rec: 0.9720 F-Score: 0.9733\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0568 Acc: 0.9790 Pre: 0.9761 Rec: 0.9820 F-Score: 0.9791\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 18/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 25/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0685 Acc: 0.9730 Pre: 0.9742 Rec: 0.9717 F-Score: 0.9730\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0570 Acc: 0.9780 Pre: 0.9761 Rec: 0.9800 F-Score: 0.9780\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 19/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 26/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0680 Acc: 0.9731 Pre: 0.9737 Rec: 0.9726 F-Score: 0.9731\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0571 Acc: 0.9780 Pre: 0.9761 Rec: 0.9800 F-Score: 0.9780\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 20/20\n",
            "\n",
            "[ğŸ•‘ TRAINING COMPLETE] 3m 26s\n",
            "[ğŸ¥‡ BEST SCORE] F-Score: 0.982178\n",
            "Using device: cuda:0\n",
            "\n",
            "NVIDIA GeForce RTX 2070\n",
            "[ğŸ’» MEMORY USAGE]\n",
            "[ğŸ“Œ ALLOCATED] 0.0 GB\n",
            "[ğŸ§® CACHED] 0.3 GB\n",
            "TRAINING WITH 40,60 DATASET RATIO\n",
            "[ğŸ’ª EPOCH] 1/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0976 Acc: 0.9626 Pre: 0.9683 Rec: 0.9694 F-Score: 0.9689\n",
            "\t[ğŸ•‘] 0m 26s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0521 Acc: 0.9790 Pre: 0.9858 Rec: 0.9720 F-Score: 0.9789\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "\n",
            "[ğŸ’ª EPOCH] 2/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0922 Acc: 0.9650 Pre: 0.9692 Rec: 0.9726 F-Score: 0.9709\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0503 Acc: 0.9810 Pre: 0.9859 Rec: 0.9760 F-Score: 0.9809\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 3/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0891 Acc: 0.9661 Pre: 0.9709 Rec: 0.9726 F-Score: 0.9717\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0490 Acc: 0.9840 Pre: 0.9859 Rec: 0.9820 F-Score: 0.9840\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 4/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0868 Acc: 0.9678 Pre: 0.9718 Rec: 0.9746 F-Score: 0.9732\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0480 Acc: 0.9850 Pre: 0.9879 Rec: 0.9820 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 5/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0850 Acc: 0.9679 Pre: 0.9723 Rec: 0.9743 F-Score: 0.9733\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0472 Acc: 0.9860 Pre: 0.9880 Rec: 0.9840 F-Score: 0.9860\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 6/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0834 Acc: 0.9678 Pre: 0.9723 Rec: 0.9740 F-Score: 0.9732\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0466 Acc: 0.9850 Pre: 0.9860 Rec: 0.9840 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 1/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 7/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0821 Acc: 0.9690 Pre: 0.9735 Rec: 0.9749 F-Score: 0.9742\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0461 Acc: 0.9860 Pre: 0.9860 Rec: 0.9860 F-Score: 0.9860\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 2/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 8/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0809 Acc: 0.9685 Pre: 0.9729 Rec: 0.9746 F-Score: 0.9737\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0458 Acc: 0.9860 Pre: 0.9860 Rec: 0.9860 F-Score: 0.9860\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 3/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 9/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0798 Acc: 0.9688 Pre: 0.9729 Rec: 0.9751 F-Score: 0.9740\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0455 Acc: 0.9870 Pre: 0.9860 Rec: 0.9880 F-Score: 0.9870\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\n",
            "[ğŸ’ª EPOCH] 10/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0788 Acc: 0.9695 Pre: 0.9732 Rec: 0.9760 F-Score: 0.9746\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0453 Acc: 0.9870 Pre: 0.9860 Rec: 0.9880 F-Score: 0.9870\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 1/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 11/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0778 Acc: 0.9698 Pre: 0.9735 Rec: 0.9763 F-Score: 0.9749\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0451 Acc: 0.9860 Pre: 0.9841 Rec: 0.9880 F-Score: 0.9860\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 2/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 12/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0769 Acc: 0.9702 Pre: 0.9738 Rec: 0.9766 F-Score: 0.9752\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0450 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 3/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 13/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0761 Acc: 0.9703 Pre: 0.9741 Rec: 0.9766 F-Score: 0.9753\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0449 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 4/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 14/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0753 Acc: 0.9702 Pre: 0.9741 Rec: 0.9763 F-Score: 0.9752\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0448 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 5/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 15/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0745 Acc: 0.9707 Pre: 0.9749 Rec: 0.9763 F-Score: 0.9756\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0448 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 6/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 16/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0738 Acc: 0.9707 Pre: 0.9752 Rec: 0.9760 F-Score: 0.9756\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0448 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 7/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 17/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0731 Acc: 0.9709 Pre: 0.9752 Rec: 0.9763 F-Score: 0.9757\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0448 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 8/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 18/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0724 Acc: 0.9714 Pre: 0.9755 Rec: 0.9769 F-Score: 0.9762\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0449 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 9/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 19/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0718 Acc: 0.9717 Pre: 0.9757 Rec: 0.9771 F-Score: 0.9764\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0449 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 10/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 20/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0712 Acc: 0.9722 Pre: 0.9758 Rec: 0.9780 F-Score: 0.9769\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0450 Acc: 0.9850 Pre: 0.9821 Rec: 0.9880 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 11/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 21/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0706 Acc: 0.9727 Pre: 0.9761 Rec: 0.9786 F-Score: 0.9773\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0450 Acc: 0.9840 Pre: 0.9802 Rec: 0.9880 F-Score: 0.9841\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 12/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 22/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0700 Acc: 0.9727 Pre: 0.9763 Rec: 0.9783 F-Score: 0.9773\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0451 Acc: 0.9840 Pre: 0.9802 Rec: 0.9880 F-Score: 0.9841\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 13/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 23/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0694 Acc: 0.9724 Pre: 0.9760 Rec: 0.9780 F-Score: 0.9770\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0451 Acc: 0.9840 Pre: 0.9802 Rec: 0.9880 F-Score: 0.9841\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 14/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 24/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0689 Acc: 0.9727 Pre: 0.9761 Rec: 0.9786 F-Score: 0.9773\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0452 Acc: 0.9840 Pre: 0.9802 Rec: 0.9880 F-Score: 0.9841\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 15/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 25/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0684 Acc: 0.9729 Pre: 0.9761 Rec: 0.9789 F-Score: 0.9775\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0453 Acc: 0.9830 Pre: 0.9782 Rec: 0.9880 F-Score: 0.9831\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 16/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 26/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0679 Acc: 0.9733 Pre: 0.9764 Rec: 0.9791 F-Score: 0.9777\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0454 Acc: 0.9820 Pre: 0.9763 Rec: 0.9880 F-Score: 0.9821\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 17/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 27/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0674 Acc: 0.9734 Pre: 0.9764 Rec: 0.9794 F-Score: 0.9779\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0455 Acc: 0.9810 Pre: 0.9744 Rec: 0.9880 F-Score: 0.9811\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 18/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 28/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0669 Acc: 0.9734 Pre: 0.9766 Rec: 0.9791 F-Score: 0.9779\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0456 Acc: 0.9810 Pre: 0.9744 Rec: 0.9880 F-Score: 0.9811\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 19/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 29/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0664 Acc: 0.9734 Pre: 0.9766 Rec: 0.9791 F-Score: 0.9779\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0457 Acc: 0.9810 Pre: 0.9744 Rec: 0.9880 F-Score: 0.9811\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 20/20\n",
            "\n",
            "[ğŸ•‘ TRAINING COMPLETE] 3m 20s\n",
            "[ğŸ¥‡ BEST SCORE] F-Score: 0.987013\n",
            "Using device: cuda:0\n",
            "\n",
            "NVIDIA GeForce RTX 2070\n",
            "[ğŸ’» MEMORY USAGE]\n",
            "[ğŸ“Œ ALLOCATED] 0.0 GB\n",
            "[ğŸ§® CACHED] 0.3 GB\n",
            "TRAINING WITH 30,70 DATASET RATIO\n",
            "[ğŸ’ª EPOCH] 1/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0787 Acc: 0.9706 Pre: 0.9764 Rec: 0.9817 F-Score: 0.9791\n",
            "\t[ğŸ•‘] 0m 23s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0490 Acc: 0.9840 Pre: 0.9764 Rec: 0.9920 F-Score: 0.9841\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "\n",
            "[ğŸ’ª EPOCH] 2/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0713 Acc: 0.9736 Pre: 0.9787 Rec: 0.9837 F-Score: 0.9812\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0494 Acc: 0.9840 Pre: 0.9764 Rec: 0.9920 F-Score: 0.9841\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 1/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 3/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0684 Acc: 0.9740 Pre: 0.9787 Rec: 0.9843 F-Score: 0.9815\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0497 Acc: 0.9830 Pre: 0.9745 Rec: 0.9920 F-Score: 0.9832\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 2/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 4/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0661 Acc: 0.9744 Pre: 0.9792 Rec: 0.9843 F-Score: 0.9818\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0500 Acc: 0.9820 Pre: 0.9744 Rec: 0.9900 F-Score: 0.9821\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 3/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 5/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0643 Acc: 0.9762 Pre: 0.9804 Rec: 0.9857 F-Score: 0.9830\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0501 Acc: 0.9810 Pre: 0.9744 Rec: 0.9880 F-Score: 0.9811\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 4/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 6/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0627 Acc: 0.9768 Pre: 0.9804 Rec: 0.9866 F-Score: 0.9835\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0502 Acc: 0.9800 Pre: 0.9724 Rec: 0.9880 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 5/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 7/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0613 Acc: 0.9770 Pre: 0.9810 Rec: 0.9863 F-Score: 0.9836\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0504 Acc: 0.9800 Pre: 0.9724 Rec: 0.9880 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 6/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 8/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0601 Acc: 0.9780 Pre: 0.9818 Rec: 0.9869 F-Score: 0.9843\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0505 Acc: 0.9800 Pre: 0.9724 Rec: 0.9880 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 7/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 9/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0590 Acc: 0.9786 Pre: 0.9824 Rec: 0.9871 F-Score: 0.9848\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0506 Acc: 0.9800 Pre: 0.9724 Rec: 0.9880 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 8/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 10/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0580 Acc: 0.9784 Pre: 0.9821 Rec: 0.9871 F-Score: 0.9846\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0508 Acc: 0.9790 Pre: 0.9705 Rec: 0.9880 F-Score: 0.9792\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 9/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 11/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0571 Acc: 0.9790 Pre: 0.9824 Rec: 0.9877 F-Score: 0.9850\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0509 Acc: 0.9790 Pre: 0.9705 Rec: 0.9880 F-Score: 0.9792\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 10/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 12/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0563 Acc: 0.9800 Pre: 0.9830 Rec: 0.9886 F-Score: 0.9858\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0511 Acc: 0.9790 Pre: 0.9705 Rec: 0.9880 F-Score: 0.9792\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 11/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 13/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0554 Acc: 0.9800 Pre: 0.9830 Rec: 0.9886 F-Score: 0.9858\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0513 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 12/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 14/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0547 Acc: 0.9802 Pre: 0.9832 Rec: 0.9886 F-Score: 0.9859\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0515 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 13/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 15/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0540 Acc: 0.9802 Pre: 0.9832 Rec: 0.9886 F-Score: 0.9859\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0517 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 14/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 16/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0533 Acc: 0.9800 Pre: 0.9830 Rec: 0.9886 F-Score: 0.9858\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0519 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 15/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 17/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0526 Acc: 0.9804 Pre: 0.9832 Rec: 0.9889 F-Score: 0.9860\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0522 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 16/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 18/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0520 Acc: 0.9804 Pre: 0.9832 Rec: 0.9889 F-Score: 0.9860\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0524 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 17/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 19/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0514 Acc: 0.9806 Pre: 0.9835 Rec: 0.9889 F-Score: 0.9862\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0526 Acc: 0.9800 Pre: 0.9706 Rec: 0.9900 F-Score: 0.9802\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 18/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 20/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0509 Acc: 0.9808 Pre: 0.9838 Rec: 0.9889 F-Score: 0.9863\n",
            "\t[ğŸ•‘] 0m 5s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0529 Acc: 0.9790 Pre: 0.9687 Rec: 0.9900 F-Score: 0.9792\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 19/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 21/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0503 Acc: 0.9812 Pre: 0.9841 Rec: 0.9891 F-Score: 0.9866\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0532 Acc: 0.9790 Pre: 0.9687 Rec: 0.9900 F-Score: 0.9792\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 20/20\n",
            "\n",
            "[ğŸ•‘ TRAINING COMPLETE] 2m 14s\n",
            "[ğŸ¥‡ BEST SCORE] F-Score: 0.984127\n",
            "Using device: cuda:0\n",
            "\n",
            "NVIDIA GeForce RTX 2070\n",
            "[ğŸ’» MEMORY USAGE]\n",
            "[ğŸ“Œ ALLOCATED] 0.0 GB\n",
            "[ğŸ§® CACHED] 0.3 GB\n",
            "TRAINING WITH 20,80 DATASET RATIO\n",
            "[ğŸ’ª EPOCH] 1/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0502 Acc: 0.9799 Pre: 0.9866 Rec: 0.9883 F-Score: 0.9874\n",
            "\t[ğŸ•‘] 0m 20s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0846 Acc: 0.9700 Pre: 0.9468 Rec: 0.9960 F-Score: 0.9708\n",
            "\t[ğŸ•‘] 0m 6s\n",
            "\n",
            "[ğŸ’ª EPOCH] 2/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0435 Acc: 0.9842 Pre: 0.9886 Rec: 0.9917 F-Score: 0.9902\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0867 Acc: 0.9680 Pre: 0.9432 Rec: 0.9960 F-Score: 0.9689\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 1/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 3/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0409 Acc: 0.9847 Pre: 0.9883 Rec: 0.9926 F-Score: 0.9904\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0882 Acc: 0.9690 Pre: 0.9450 Rec: 0.9960 F-Score: 0.9698\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 2/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 4/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0389 Acc: 0.9858 Pre: 0.9895 Rec: 0.9929 F-Score: 0.9912\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0895 Acc: 0.9660 Pre: 0.9396 Rec: 0.9960 F-Score: 0.9670\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 3/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 5/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0373 Acc: 0.9861 Pre: 0.9895 Rec: 0.9931 F-Score: 0.9913\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0906 Acc: 0.9660 Pre: 0.9396 Rec: 0.9960 F-Score: 0.9670\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 4/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 6/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0359 Acc: 0.9863 Pre: 0.9897 Rec: 0.9931 F-Score: 0.9914\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0915 Acc: 0.9660 Pre: 0.9396 Rec: 0.9960 F-Score: 0.9670\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 5/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 7/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0347 Acc: 0.9865 Pre: 0.9900 Rec: 0.9931 F-Score: 0.9916\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0923 Acc: 0.9650 Pre: 0.9395 Rec: 0.9940 F-Score: 0.9660\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 6/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 8/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0336 Acc: 0.9874 Pre: 0.9906 Rec: 0.9937 F-Score: 0.9922\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0930 Acc: 0.9660 Pre: 0.9413 Rec: 0.9940 F-Score: 0.9669\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 7/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 9/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0326 Acc: 0.9879 Pre: 0.9909 Rec: 0.9940 F-Score: 0.9924\n",
            "\t[ğŸ•‘] 0m 4s\n",
            "[ğŸ—ƒï¸ VAL] Loss: 0.0936 Acc: 0.9650 Pre: 0.9395 Rec: 0.9940 F-Score: 0.9660\n",
            "\t[ğŸ•‘] 0m 1s\n",
            "\t[âš ï¸ EARLY STOPPING] 8/20\n",
            "\n",
            "[ğŸ’ª EPOCH] 10/500\n",
            "----------\n",
            "[ğŸ—ƒï¸ TRAIN] Loss: 0.0317 Acc: 0.9881 Pre: 0.9909 Rec: 0.9943 F-Score: 0.9926\n",
            "\t[ğŸ•‘] 0m 4s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-13-933807baa376>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TRAINING WITH {} DATASET RATIO\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossible_ratios\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     model_ft, scores_history, best_score, best_f1 = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,\n\u001b[0m\u001b[0;32m     20\u001b[0m       \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"inception\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         delta=delta_es, patience=patience_es)\n",
            "\u001b[1;32m<ipython-input-8-0c51acf19d33>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception, delta, patience)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;31m# statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mlabels_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mlabels_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "if not all_ratios:\n",
        "  set_seed(SEED)\n",
        "  model_ft, scores_history = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,\n",
        "    num_epochs=num_epochs, is_inception=(model_name==\"inception\"),\n",
        "      delta=delta_es, patience=patience_es)\n",
        "\n",
        "  print_gpu_stats()\n",
        "else:\n",
        "  all_models = []\n",
        "  all_best_score = []\n",
        "  all_best_f1 = []\n",
        "  for dataloaders_dict in all_dataloaders_dict:\n",
        "    i = all_dataloaders_dict.index(dataloaders_dict)\n",
        "    print(\"TRAINING WITH {} DATASET RATIO\".format(possible_ratios[i]))\n",
        "    set_seed(SEED)\n",
        "    model_ft, scores_history, best_score, best_f1 = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,\n",
        "      num_epochs=num_epochs, is_inception=(model_name==\"inception\"),\n",
        "        delta=delta_es, patience=patience_es)\n",
        "    all_models.append(model_ft)\n",
        "    all_best_score.append(best_score)\n",
        "    all_best_f1.append(best_f1)\n",
        "\n",
        "    print_gpu_stats()\n",
        "\n",
        "  with open(\"best_scores.txt\", 'w') as file:\n",
        "        for best_score in all_best_score:\n",
        "            s = \" \".join(map(str, best_score))\n",
        "            file.write(s+'\\n')\n",
        "\n",
        "  with open(\"best_f1.txt\", 'w') as file:\n",
        "        for best_f1 in all_best_f1:\n",
        "            s = \" \".join(map(str, best_f1))\n",
        "            file.write(s+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-3CFY32Lv9j"
      },
      "outputs": [],
      "source": [
        "if not all_ratios:\n",
        "  if save_model:\n",
        "    torch.save(model_ft, model_save_path + \"_weights.pt\")\n",
        "    print(\"[ğŸ’¾ SAVED] Weights\")\n",
        "\n",
        "  if save_entire_model:\n",
        "    torch.save(model_ft.state_dict(), model_save_path + \".pt\")\n",
        "    print(\"[ğŸ’¾ SAVED] Entire model\")\n",
        "\n",
        "  if save_all:\n",
        "    torch.save({\n",
        "      'model': model_ft,\n",
        "      'dataset': dataset_dir,\n",
        "      'learning_rate': learning_rate,\n",
        "      'momentum': momentum,\n",
        "      'dataset_sizes': dataset_sizes,\n",
        "      'model_name': model_name,\n",
        "      'batch_size': batch_size,\n",
        "      'num_epochs': num_epochs,\n",
        "      'criterion': criterion,\n",
        "      'optimizer': optimizer_ft,\n",
        "      'scores_history': scores_history,\n",
        "      'delta_es': delta_es,\n",
        "      'patience_es': patience_es\n",
        "    }, model_save_path + \"_all.pt\")\n",
        "\n",
        "    print(\"[ğŸ’¾ SAVED] All\")\n",
        "else:\n",
        "  for i in range(len(possible_ratios)):\n",
        "    if save_model:\n",
        "      torch.save(all_models[i], model_save_path + \"_weights.pt\")\n",
        "      print(\"[ğŸ’¾ SAVED] Weights\")\n",
        "\n",
        "    if save_entire_model:\n",
        "      torch.save(all_models[i].state_dict(), model_save_path + \".pt\")\n",
        "      print(\"[ğŸ’¾ SAVED] Entire model\")\n",
        "\n",
        "    if save_all:\n",
        "      torch.save({\n",
        "        'model': all_models[i],\n",
        "        'dataset': dataset_dir,\n",
        "        'learning_rate': learning_rate,\n",
        "        'momentum': momentum,\n",
        "        'dataset_sizes': all_dataset_sizes[i],\n",
        "        'model_name': model_name,\n",
        "        'batch_size': batch_size,\n",
        "        'num_epochs': num_epochs,\n",
        "        'criterion': criterion,\n",
        "        'optimizer': optimizer_ft,\n",
        "        'scores_history': scores_history,\n",
        "        'delta_es': delta_es,\n",
        "        'patience_es': patience_es\n",
        "      }, model_save_path + \"_\" + possible_ratios[all_dataset_sizes.index(dataset_sizes)] \"_all.pt\")\n",
        "\n",
        "      print(\"[ğŸ’¾ SAVED] All\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "featureExtractor_DDG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
