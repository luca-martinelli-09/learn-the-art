{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluator\n",
    "Evaluate all the models on all the test datasets\n",
    "\n",
    "**Authors**\n",
    "\n",
    "`Marco Alecci <https://github.com/MarcoAlecci>`\n",
    "\n",
    "`Francesco Marchiori <https://github.com/FrancescoMarchiori>`\n",
    "\n",
    "`Luca Martinelli <https://github.com/luca-martinelli-09>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luca-martinelli-09/orco-gan/blob/main/modelEvaluator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"./datasets\"):\n",
    "    !git clone \"https://github.com/luca-martinelli-09/orco-gan.git\"\n",
    "\n",
    "    %cd orco-gan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "googleModelsDir = None\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  googleModelsDir = \"/content/drive/MyDrive/UniversitaÃÄ/Magistrale/II Anno/I Semestre/Advanced Topics in Computer and Network Security/Project/Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a manual seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 151836\n",
    "\n",
    "def setSeed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "setSeed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGPUStats():\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "\n",
    "    # Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('[üíª MEMORY USAGE]')\n",
    "        print('[üìå ALLOCATED]', round(\n",
    "            torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
    "        print('[üßÆ CACHED]', round(\n",
    "            torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubDirs(dir):\n",
    "    return [x for x in os.listdir(dir) if os.path.isdir(os.path.join(dir, x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassPercents(sizes):\n",
    "    totalSize = np.sum(np.array(sizes))\n",
    "    percents = []\n",
    "    for size in sizes:\n",
    "        percents.append(int(round((size / totalSize) * 100)))\n",
    "    \n",
    "    return percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestScores(hist, key, min=False):\n",
    "    scores = [x[key] for x in hist]\n",
    "\n",
    "    if min:\n",
    "        i = np.argmin(np.array(scores))\n",
    "    else:\n",
    "        i = np.argmax(np.array(scores))\n",
    "\n",
    "    return hist[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanAndSDT(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsDir = \"./datasets\"\n",
    "modelsDir = googleModelsDir if googleModelsDir else \"./models\"\n",
    "adversarialsDir = \"./adversarial_samples\"\n",
    "\n",
    "inputSize = 224 # Specified for alexnet, resnet, vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(labels, predicted):\n",
    "    acc = torch.sum(predicted == labels) / len(predicted)\n",
    "\n",
    "    tp = (labels * predicted).sum()\n",
    "    tn = ((1 - labels) * (1 - predicted)).sum()\n",
    "    fp = ((1 - labels) * predicted).sum()\n",
    "    fn = (labels * (1 - predicted)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, dataloader):\n",
    "    model.eval()\n",
    "    labelsOutputs = torch.tensor([]).to(device, non_blocking=True)\n",
    "    labelsTargets = torch.tensor([]).to(device, non_blocking=True)\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        labelsOutputs = torch.cat([labelsOutputs, preds], dim=0)\n",
    "        labelsTargets = torch.cat([labelsTargets, labels], dim=0)\n",
    "\n",
    "    acc, precision, recall, f1 = getScores(labelsTargets, labelsOutputs)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc.cpu().numpy(),\n",
    "        \"precision\": precision.cpu().numpy(),\n",
    "        \"recall\": recall.cpu().numpy(),\n",
    "        \"f1\": f1.cpu().numpy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informations about models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[üß† MODELS INFORMATION]\")\n",
    "\n",
    "modelsInformation = []\n",
    "\n",
    "for dataset in getSubDirs(modelsDir):\n",
    "    print(\"\\n\" + \"-\" * 15)\n",
    "    print(\"[üóÉÔ∏è DATASET] {}\".format(dataset))\n",
    "\n",
    "    datasetDir = os.path.join(modelsDir, dataset)\n",
    "\n",
    "    for modelType in getSubDirs(datasetDir):\n",
    "        print(\"\\n[üßÆ MODEL TYPE] {}\".format(modelType))\n",
    "\n",
    "        modelsTypeDir = os.path.join(datasetDir, modelType)\n",
    "        \n",
    "        for model in os.listdir(modelsTypeDir):\n",
    "            print(\"\\n\\t[üß† MODEL] {}\".format(model))\n",
    "\n",
    "            path = os.path.join(modelsTypeDir, model)\n",
    "\n",
    "            checkpoint = torch.load(path)\n",
    "            \n",
    "            bestScore, i = getBestScores(checkpoint[\"scores_history\"], \"f1\")\n",
    "            classBalancing = getClassPercents(checkpoint[\"dataset_sizes\"])\n",
    "            balancingStr = \"/\".join([str(x) for x in classBalancing])\n",
    "\n",
    "            modelsInformation.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"model\": checkpoint[\"model_name\"],\n",
    "                \"epochs\": len(checkpoint[\"scores_history\"]),\n",
    "                \"balancing\": balancingStr,\n",
    "                \"f-score\": bestScore[\"f1\"],\n",
    "            })\n",
    "\n",
    "            print(\"\\tModel:\", checkpoint[\"model_name\"])\n",
    "            print(\"\\tEpochs:\", len(checkpoint[\"scores_history\"]))\n",
    "            print(\"\\tBalancing:\", classBalancing)\n",
    "            print(\"\\tBest epoch:\", i)\n",
    "            print(\"\\tBest F-Score:\", bestScore[\"f1\"])\n",
    "            print(\"\\tHistory:\", [float(x[\"f1\"]) for x in checkpoint[\"scores_history\"]])\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "modelsInformationDF = pd.DataFrame(modelsInformation)\n",
    "\n",
    "printGPUStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsInformationDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModelsOnDataset(datasetFolder, datasetInfo):\n",
    "    global modelsDir, inputSize\n",
    "\n",
    "    modelsEvals = []\n",
    "\n",
    "    # Get the images and calculate mean and standard deviation\n",
    "    imageDataset = torchvision.datasets.ImageFolder(\n",
    "        datasetFolder, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "        \n",
    "    for cls in imageDataset.classes:\n",
    "        cls_index = imageDataset.class_to_idx[cls]\n",
    "        num_cls = np.count_nonzero(\n",
    "            np.array(imageDataset.targets) == cls_index)\n",
    "        \n",
    "        print(\"\\t[üßÆ # ELEMENTS] {}: {}\".format(cls, num_cls))\n",
    "    \n",
    "    imageDataloader = DataLoader(imageDataset, batch_size=128)\n",
    "    \n",
    "    mean, std = getMeanAndSDT(imageDataloader)\n",
    "\n",
    "    # Setup for normalization\n",
    "    dataTransform = transforms.Compose([\n",
    "        transforms.Resize(inputSize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    testDataset = ImageLimitedDataset(\n",
    "        datasetFolder, transform=dataTransform, use_cache=True, check_images=False)\n",
    "\n",
    "    setSeed(SEED)\n",
    "    testDataLoader = DataLoader(\n",
    "        testDataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Evaluate every model\n",
    "    for root, _, fnames in sorted(os.walk(modelsDir, followlinks=True)):\n",
    "        for fname in sorted(fnames):\n",
    "            path = os.path.join(root, fname)\n",
    "\n",
    "            try:\n",
    "                modelData = torch.load(path)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            modelDataset = modelData[\"dataset\"]\n",
    "            modelName = modelData[\"model_name\"]\n",
    "            modelPercents = \"/\".join([str(x)\n",
    "                                     for x in getClassPercents(modelData[\"dataset_sizes\"])])\n",
    "\n",
    "            print()\n",
    "            print(\"[üßÆ EVALUATING] {} - {} {}\".format(\n",
    "                modelDataset,\n",
    "                modelName,\n",
    "                modelPercents\n",
    "            ))\n",
    "\n",
    "            modelToTest = modelData[\"model\"]\n",
    "            modelToTest = modelToTest.to(device, non_blocking=True)\n",
    "\n",
    "            scores = evaluateModel(modelToTest, testDataLoader)\n",
    "\n",
    "            modelsEvals.append({\n",
    "                    \"dataset\": datasetInfo[\"dataset\"],\n",
    "                    \"isMath\": datasetInfo[\"math\"],\n",
    "                    \"attack\": datasetInfo[\"attack\"],\n",
    "                    \"advModel\": datasetInfo[\"model\"],\n",
    "                    \"advBalancing\": datasetInfo[\"balancing\"],\n",
    "\n",
    "                    \"model\": modelName,\n",
    "                    \"modelDataset\": modelDataset,\n",
    "                    \"balancing\": modelPercents,\n",
    "                    \"acc\": scores[\"acc\"],\n",
    "                    \"precision\": scores[\"precision\"],\n",
    "                    \"recall\": scores[\"recall\"],\n",
    "                    \"f1\": scores[\"f1\"],\n",
    "                })\n",
    "            \n",
    "            print(\"\\tAcc: {:.4f}\".format(scores[\"acc\"]))\n",
    "            print(\"\\tPre: {:.4f}\".format(scores[\"precision\"]))\n",
    "            print(\"\\tRec: {:.4f}\".format(scores[\"recall\"]))\n",
    "            print(\"\\tF-Score: {:.4f}\".format(scores[\"f1\"]))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    return modelsEvals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageLimitedDataset import ImageLimitedDataset\n",
    "\n",
    "modelsEvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[üß† MODELS EVALUATION - NO ATTACKS]\")\n",
    "\n",
    "# Evaluate models on test folders\n",
    "for dataset in getSubDirs(datasetsDir):\n",
    "    print(\"\\n\" + \"-\" * 15)\n",
    "    print(\"[üóÉÔ∏è TEST DATASET] {}\".format(dataset))\n",
    "    \n",
    "    datasetDir = os.path.join(datasetsDir, dataset)\n",
    "    testDir = os.path.join(datasetDir, \"test\")\n",
    "\n",
    "    advDatasetInfo = {\n",
    "        \"dataset\": dataset,\n",
    "        \"math\": None,\n",
    "        \"attack\": None,\n",
    "        \"balancing\": None,\n",
    "        \"model\": None,\n",
    "    }\n",
    "\n",
    "    evals = evaluateModelsOnDataset(testDir, advDatasetInfo)\n",
    "    modelsEvals.extend(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[üß† MODELS EVALUATION - NON MATH ATTACKS]\")\n",
    "\n",
    "# Evaluate models on non math attacks folders\n",
    "for dataset in getSubDirs(adversarialsDir):\n",
    "    datasetDir = os.path.join(adversarialsDir, dataset)\n",
    "    nonMathAdvDir = os.path.join(datasetDir, \"nonMath\")\n",
    "\n",
    "    for attack in getSubDirs(nonMathAdvDir):\n",
    "        print(\"\\n\" + \"-\" * 15)\n",
    "        print(\"[üóÉÔ∏è ADVERSARIAL DATASET] {}/{}\".format(dataset, attack))\n",
    "\n",
    "        attackDir = os.path.join(nonMathAdvDir, attack)\n",
    "\n",
    "        advDatasetInfo = {\n",
    "            \"dataset\": dataset,\n",
    "            \"math\": False,\n",
    "            \"attack\": attack,\n",
    "            \"balancing\": None,\n",
    "            \"model\": None,\n",
    "        }\n",
    "\n",
    "        evals = evaluateModelsOnDataset(attackDir, advDatasetInfo)\n",
    "        modelsEvals.extend(evals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[üß† MODELS EVALUATION - MATH ATTACKS]\")\n",
    "\n",
    "# Evaluate models on math attacks folders\n",
    "for dataset in getSubDirs(adversarialsDir):\n",
    "    datasetDir = os.path.join(adversarialsDir, dataset)\n",
    "    mathAdvDir = os.path.join(datasetDir, \"math\")\n",
    "\n",
    "    if not os.path.exists(mathAdvDir):\n",
    "        continue\n",
    "\n",
    "    for attack in getSubDirs(mathAdvDir):\n",
    "        attackDir = os.path.join(mathAdvDir, attack)\n",
    "\n",
    "        for advModel in getSubDirs(attackDir):\n",
    "            advModelDir = os.path.join(attackDir, advModel)\n",
    "\n",
    "            for advBalancing in getSubDirs(advModelDir):\n",
    "                advDatasetDir = os.path.join(advModelDir, advBalancing)\n",
    "\n",
    "                print(\"\\n\" + \"-\" * 15)\n",
    "                print(\"[üóÉÔ∏è ADVERSARIAL DATASET] {}/{}/{}/{}\".format(dataset, attack, advModel, advBalancing))\n",
    "\n",
    "                advDatasetInfo = {\n",
    "                    \"dataset\": dataset,\n",
    "                    \"math\": True,\n",
    "                    \"attack\": attack,\n",
    "                    \"balancing\": advBalancing.replace(\"_\", \"/\"),\n",
    "                    \"model\": advModel,\n",
    "                }\n",
    "\n",
    "                evals = evaluateModelsOnDataset(advDatasetDir, advDatasetInfo)\n",
    "                modelsEvals.extend(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF = pd.DataFrame(modelsEvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF.to_csv(\"modelsEvaluations.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
