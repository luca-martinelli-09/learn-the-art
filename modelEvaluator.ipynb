{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluator\n",
    "Evaluate all the models on all the test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luca-martinelli-09/orco-gan/blob/main/modelEvaluator.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ## Setup project\n",
    "# @markdown This section will download the datasets from GitHub to use for the training phase\n",
    "\n",
    "if not os.path.exists(\"./datasets\"):\n",
    "    !git clone \"https://github.com/luca-martinelli-09/orco-gan.git\"\n",
    "\n",
    "    %cd orco-gan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchattacks\n",
    "from torchattacks import PGD, FGSM\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)\n",
    "print(\"Torchattacks\", torchattacks.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a manual seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 151836\n",
    "\n",
    "def setSeed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "setSeed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGPUStats():\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "\n",
    "    # Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('[üíª MEMORY USAGE]')\n",
    "        print('[üìå ALLOCATED]', round(\n",
    "            torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
    "        print('[üßÆ CACHED]', round(\n",
    "            torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubDirs(dir):\n",
    "    return [x for x in os.listdir(dir) if os.path.isdir(os.path.join(dir, x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassPercents(sizes):\n",
    "    totalSize = np.sum(np.array(sizes))\n",
    "    percents = []\n",
    "    for size in sizes:\n",
    "        percents.append(int(round((size / totalSize) * 100)))\n",
    "    \n",
    "    return percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestScores(hist, key, min=False):\n",
    "    scores = [x[key] for x in hist]\n",
    "\n",
    "    if min:\n",
    "        i = np.argmin(np.array(scores))\n",
    "    else:\n",
    "        i = np.argmax(np.array(scores))\n",
    "\n",
    "    return hist[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanAndSDT(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Datasets\n",
    "datasetsDir = \"./datasets\" # @param {type: \"string\"}\n",
    "\n",
    "# @markdown ### Models\n",
    "modelsDir = \"./models\" # @param {type: \"string\"}\n",
    "\n",
    "inputSize = 224 # Specified for alexnet, resnet, vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(labels, predicted):\n",
    "    acc = torch.sum(predicted == labels) / len(predicted)\n",
    "\n",
    "    tp = (labels * predicted).sum()\n",
    "    tn = ((1 - labels) * (1 - predicted)).sum()\n",
    "    fp = ((1 - labels) * predicted).sum()\n",
    "    fn = (labels * (1 - predicted)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, dataloader, attack=None):\n",
    "    model.eval()\n",
    "    labelsOutputs = torch.tensor([]).to(device, non_blocking=True)\n",
    "    labelsTargets = torch.tensor([]).to(device, non_blocking=True)\n",
    "\n",
    "    setSeed(SEED)\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = attack(inputs, labels) if attack else inputs\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        labelsOutputs = torch.cat([labelsOutputs, preds], dim=0)\n",
    "        labelsTargets = torch.cat([labelsTargets, labels], dim=0)\n",
    "    \n",
    "    acc, precision, recall, f1 = getScores(labelsTargets, labelsOutputs)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc.cpu().numpy(),\n",
    "        \"precision\": precision.cpu().numpy(),\n",
    "        \"recall\": recall.cpu().numpy(),\n",
    "        \"f1\": f1.cpu().numpy()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std) :\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Broadcasting\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informations about models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[üß† MODELS INFORMATION]\")\n",
    "\n",
    "modelsInformation = []\n",
    "\n",
    "for dataset in getSubDirs(modelsDir):\n",
    "    print(\"\\n\" + \"-\" * 15)\n",
    "    print(\"[üóÉÔ∏è DATASET] {}\".format(dataset))\n",
    "\n",
    "    datasetDir = os.path.join(modelsDir, dataset)\n",
    "\n",
    "    for modelType in getSubDirs(datasetDir):\n",
    "        print(\"\\n[üßÆ MODEL TYPE] {}\".format(modelType))\n",
    "\n",
    "        modelsTypeDir = os.path.join(datasetDir, modelType)\n",
    "        \n",
    "        for model in os.listdir(modelsTypeDir):\n",
    "            print(\"\\n\\t[üß† MODEL] {}\".format(model))\n",
    "\n",
    "            path = os.path.join(modelsTypeDir, model)\n",
    "\n",
    "            checkpoint = torch.load(path)\n",
    "            \n",
    "            bestScore, i = getBestScores(checkpoint[\"scores_history\"], \"f1\")\n",
    "            classBalancing = getClassPercents(checkpoint[\"dataset_sizes\"])\n",
    "            balancingStr = \"/\".join([str(x) for x in classBalancing])\n",
    "\n",
    "            modelsInformation.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"model\": checkpoint[\"model_name\"],\n",
    "                \"epochs\": len(checkpoint[\"scores_history\"]),\n",
    "                \"balancing\": balancingStr,\n",
    "                \"f-score\": bestScore[\"f1\"],\n",
    "            })\n",
    "\n",
    "            print(\"\\tModel:\", checkpoint[\"model_name\"])\n",
    "            print(\"\\tEpochs:\", len(checkpoint[\"scores_history\"]))\n",
    "            print(\"\\tBalancing:\", classBalancing)\n",
    "            print(\"\\tBest epoch:\", i)\n",
    "            print(\"\\tBest F-Score:\", bestScore[\"f1\"])\n",
    "            print(\"\\tHistory:\", [float(x[\"f1\"]) for x in checkpoint[\"scores_history\"]])\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "modelsInformationDF = pd.DataFrame(modelsInformation)\n",
    "\n",
    "printGPUStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsInformationDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterAttacks = {\n",
    "    \"none\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageLimitedDataset import ImageLimitedDataset\n",
    "\n",
    "print(\"[üß† MODELS EVALUATION]\")\n",
    "\n",
    "modelsEvals = []\n",
    "\n",
    "for dataset in getSubDirs(datasetsDir):\n",
    "    print(\"\\n\" + \"-\" * 15)\n",
    "    print(\"[üóÉÔ∏è TEST DATASET] {}\".format(dataset))\n",
    "    \n",
    "    datasetDir = os.path.join(datasetsDir, dataset)\n",
    "    testDir = os.path.join(datasetDir, \"test\")\n",
    "\n",
    "    for filterAttack in filterAttacks:\n",
    "        print(\"[‚öîÔ∏è FILTER ATTACK] {}\".format(filterAttack))\n",
    "\n",
    "        filterAttacker = filterAttacks[filterAttack]\n",
    "\n",
    "        # Get the images and calculate mean and standard deviation\n",
    "        toTensor = transforms.Compose([transforms.ToTensor()])\n",
    "        testDataset = ImageLimitedDataset(\n",
    "            testDir, transform=toTensor, use_cache=False, check_images=False, transform_filter=filterAttacker)\n",
    "\n",
    "        for cls in testDataset.classes:\n",
    "            cls_index = testDataset.class_to_idx[cls]\n",
    "            num_cls = np.count_nonzero(\n",
    "                np.array(testDataset.targets) == cls_index)\n",
    "            print(\"\\t[üßÆ # ELEMENTS] {}: {}\".format(cls, num_cls))\n",
    "\n",
    "        setSeed(SEED)\n",
    "        testDataLoader = DataLoader(testDataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "        mean, std = getMeanAndSDT(testDataLoader)\n",
    "        \n",
    "        # Setup for normalization\n",
    "        dataTransform = transforms.Compose([\n",
    "            transforms.Resize(inputSize),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        testDataset = ImageLimitedDataset(\n",
    "            testDir, transform=dataTransform, use_cache=True, check_images=False, transform_filter=filterAttacker)\n",
    "        \n",
    "        setSeed(SEED)\n",
    "        testDataLoader = DataLoader(\n",
    "            testDataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(modelsDir, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = os.path.join(root, fname)\n",
    "                model = torch.load(path)\n",
    "\n",
    "                modelDataset = model[\"dataset\"] if \"dataset\" in model.keys() else root.split(os.sep)[1]\n",
    "\n",
    "                modelPercents = \"/\".join([str(x) for x in getClassPercents(model[\"dataset_sizes\"])])\n",
    "\n",
    "                print()\n",
    "                print(\"[üßÆ EVALUATING] {} - {} {}\".format(\n",
    "                    modelDataset,\n",
    "                    model[\"model_name\"],\n",
    "                    modelPercents\n",
    "                ))\n",
    "\n",
    "                modelToTest = model[\"model\"]\n",
    "\n",
    "                # Add normalization\n",
    "                modelToTest = nn.Sequential(\n",
    "                    Normalize(mean, std),\n",
    "                    modelToTest\n",
    "                ).to(device, non_blocking=True)\n",
    "\n",
    "                if not filterAttacker:\n",
    "                    attacks = {\n",
    "                        \"none\": None,\n",
    "                        \"FGSM\": FGSM(modelToTest, eps=0.3)\n",
    "                    }\n",
    "                else:\n",
    "                    attacks = {\"none\": None}\n",
    "\n",
    "                for attack in attacks:\n",
    "                    print(\"[‚öîÔ∏è ATTACK] {}\".format(attack))\n",
    "\n",
    "                    attacker = attacks[attack]\n",
    "\n",
    "                    scores = evaluateModel(modelToTest, testDataLoader, attacker)\n",
    "\n",
    "                    modelsEvals.append({\n",
    "                            \"dataset\": dataset,\n",
    "                            \"model\": model[\"model_name\"],\n",
    "                            \"attack\": attack,\n",
    "                            \"filter\": filterAttack,\n",
    "                            \"model_dataset\": modelDataset,\n",
    "                            \"balancing\": modelPercents,\n",
    "                            \"acc\": scores[\"acc\"],\n",
    "                            \"precision\": scores[\"precision\"],\n",
    "                            \"recall\": scores[\"recall\"],\n",
    "                            \"f1\": scores[\"f1\"],\n",
    "                        })\n",
    "                    \n",
    "                    print(\"\\tAcc: {:.4f}\".format(scores[\"acc\"]))\n",
    "                    print(\"\\tPre: {:.4f}\".format(scores[\"precision\"]))\n",
    "                    print(\"\\tRec: {:.4f}\".format(scores[\"recall\"]))\n",
    "                    print(\"\\tF-Score: {:.4f}\".format(scores[\"f1\"]))\n",
    "\n",
    "                    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF = pd.DataFrame(modelsEvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF.to_csv(\"modelsEvaluations.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
