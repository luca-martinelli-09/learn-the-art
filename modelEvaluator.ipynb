{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Evaluator\n",
    "Evaluate all the models on all the test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luca-martinelli-09/orco-gan/blob/main/modelEvaluator.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ## Setup project\n",
    "# @markdown This section will download the datasets from GitHub to use for the training phase\n",
    "\n",
    "if not os.path.exists(\"./datasets\"):\n",
    "    !git clone \"https://github.com/luca-martinelli-09/orco-gan.git\"\n",
    "\n",
    "    %cd orco-gan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.10.1+cu113\n",
      "Torchvision Version: 0.11.2+cu113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a manual seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 151836\n",
    "\n",
    "def setSeed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "setSeed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGPUStats():\n",
    "    print('Using device:', device)\n",
    "    print()\n",
    "\n",
    "    # Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('[üíª MEMORY USAGE]')\n",
    "        print('[üìå ALLOCATED]', round(\n",
    "            torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
    "        print('[üßÆ CACHED]', round(\n",
    "            torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubDirs(dir):\n",
    "    return [x for x in os.listdir(dir) if os.path.isdir(os.path.join(dir, x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassPercents(sizes):\n",
    "    totalSize = np.sum(np.array(sizes))\n",
    "    percents = []\n",
    "    for size in sizes:\n",
    "        percents.append(int((size / totalSize) * 100))\n",
    "    \n",
    "    return percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestScores(hist, key, min=False):\n",
    "    scores = [x[key] for x in hist]\n",
    "\n",
    "    if min:\n",
    "        i = np.argmin(np.array(scores))\n",
    "    else:\n",
    "        i = np.argmax(np.array(scores))\n",
    "\n",
    "    return hist[i], i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown ### Datasets\n",
    "datasetsDir = \"./datasets\" # @param {type: \"string\"}\n",
    "\n",
    "# @markdown ### Models\n",
    "modelsDir = \"./models\" # @param {type: \"string\"}\n",
    "\n",
    "inputSize = 224 # Specified for alexnet, resnet, vgg\n",
    "\n",
    "# Normalization values\n",
    "normalizationVals = {\n",
    "    \"bing\": {\n",
    "        \"train\": [[0.5407, 0.5059, 0.4523], [0.2830, 0.2794, 0.2898]],\n",
    "        \"val\": [[0.5341, 0.5012, 0.4385], [0.2809, 0.2752, 0.2863]],\n",
    "        \"test\": [[0.5257, 0.4953, 0.4290], [0.2799, 0.2730, 0.2844]]\n",
    "    },\n",
    "    \"ddg\": {\n",
    "        \"train\": [[0.5366, 0.5061, 0.4544], [0.2860, 0.2820, 0.2917]],\n",
    "        \"val\": [[0.5364, 0.5036, 0.4522], [0.2868, 0.2817, 0.2917]],\n",
    "        \"test\": [[0.5323, 0.5006, 0.4465], [0.2825, 0.2784, 0.2881]]\n",
    "    },\n",
    "    \"google\": {\n",
    "        \"train\": [[0.5635, 0.5371, 0.4781], [0.2899, 0.2861, 0.3035]],\n",
    "        \"val\": [[0.5653, 0.5397, 0.4751], [0.2872, 0.2835, 0.3018]],\n",
    "        \"test\": [[0.5736, 0.5468, 0.4893], [0.2954, 0.2914, 0.3083]]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(labels, predicted):\n",
    "    acc = torch.sum(predicted == labels) / len(predicted)\n",
    "\n",
    "    tp = (labels * predicted).sum()\n",
    "    tn = ((1 - labels) * (1 - predicted)).sum()\n",
    "    fp = ((1 - labels) * predicted).sum()\n",
    "    fn = (labels * (1 - predicted)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, dataloader):\n",
    "    model.eval()\n",
    "    labelsOutputs = torch.tensor([]).to(device, non_blocking=True)\n",
    "    labelsTargets = torch.tensor([]).to(device, non_blocking=True)\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        labelsOutputs = torch.cat([labelsOutputs, preds], dim=0)\n",
    "        labelsTargets = torch.cat([labelsTargets, labels], dim=0)\n",
    "    \n",
    "    acc, precision, recall, f1 = getScores(labelsTargets, labelsOutputs)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc.cpu().numpy(),\n",
    "        \"precision\": precision.cpu().numpy(),\n",
    "        \"recall\": recall.cpu().numpy(),\n",
    "        \"f1\": f1.cpu().numpy()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informations about models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[üß† MODELS INFORMATION]\n",
      "\n",
      "---------------\n",
      "[üóÉÔ∏è DATASET] bing\n",
      "\n",
      "[üßÆ MODEL TYPE] alexnet\n",
      "\n",
      "[üßÆ MODEL TYPE] resnet\n",
      "\n",
      "[üßÆ MODEL TYPE] vgg\n",
      "\n",
      "---------------\n",
      "[üóÉÔ∏è DATASET] ddg\n",
      "\n",
      "[üßÆ MODEL TYPE] alexnet\n",
      "\n",
      "[üßÆ MODEL TYPE] resnet\n",
      "\n",
      "\t[üß† MODEL] resnet_50_50_2_all.pt\n",
      "\tModel: resnet\n",
      "\tEpochs: 24\n",
      "\tBalancing: [50, 50]\n",
      "\tBest epoch: 14\n",
      "\tBest F-Score: 0.9771144\n",
      "\tHistory: [0.9276437759399414, 0.9706180095672607, 0.966360867023468, 0.9770230054855347, 0.9722222089767456, 0.9760478138923645, 0.9749247431755066, 0.9767910838127136, 0.9738430976867676, 0.9758065342903137, 0.9747729301452637, 0.9747219085693359, 0.9736308455467224, 0.9706774353981018, 0.9771143794059753, 0.9768844246864319, 0.9738430976867676, 0.9748237133026123, 0.9716024398803711, 0.9730269312858582, 0.9686552286148071, 0.9676768183708191, 0.9694581031799316, 0.9574247598648071]\n",
      "\n",
      "[üßÆ MODEL TYPE] vgg\n",
      "\n",
      "---------------\n",
      "[üóÉÔ∏è DATASET] google\n",
      "\n",
      "[üßÆ MODEL TYPE] alexnet\n",
      "\n",
      "[üßÆ MODEL TYPE] resnet\n",
      "\n",
      "[üßÆ MODEL TYPE] vgg\n",
      "Using device: cuda:0\n",
      "\n",
      "NVIDIA GeForce GTX 1050\n",
      "[üíª MEMORY USAGE]\n",
      "[üìå ALLOCATED] 0.0 GB\n",
      "[üßÆ CACHED] 0.1 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"[üß† MODELS INFORMATION]\")\n",
    "\n",
    "modelsInformation = []\n",
    "\n",
    "for dataset in getSubDirs(modelsDir):\n",
    "    print(\"\\n\" + \"-\" * 15)\n",
    "    print(\"[üóÉÔ∏è DATASET] {}\".format(dataset))\n",
    "\n",
    "    datasetDir = os.path.join(modelsDir, dataset)\n",
    "\n",
    "    for modelType in getSubDirs(datasetDir):\n",
    "        print(\"\\n[üßÆ MODEL TYPE] {}\".format(modelType))\n",
    "\n",
    "        modelsTypeDir = os.path.join(datasetDir, modelType)\n",
    "        \n",
    "        for model in os.listdir(modelsTypeDir):\n",
    "            print(\"\\n\\t[üß† MODEL] {}\".format(model))\n",
    "\n",
    "            path = os.path.join(modelsTypeDir, model)\n",
    "\n",
    "            checkpoint = torch.load(path)\n",
    "            \n",
    "            bestScore, i = getBestScores(checkpoint[\"scores_history\"], \"f1\")\n",
    "            classBalancing = getClassPercents(checkpoint[\"dataset_sizes\"])\n",
    "            balancingStr = \"/\".join([str(x) for x in classBalancing])\n",
    "\n",
    "            modelsInformation.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"model\": checkpoint[\"model_name\"],\n",
    "                \"epochs\": len(checkpoint[\"scores_history\"]),\n",
    "                \"balancing\": balancingStr,\n",
    "                \"f-score\": bestScore[\"f1\"],\n",
    "            })\n",
    "\n",
    "            print(\"\\tModel:\", checkpoint[\"model_name\"])\n",
    "            print(\"\\tEpochs:\", len(checkpoint[\"scores_history\"]))\n",
    "            print(\"\\tBalancing:\", classBalancing)\n",
    "            print(\"\\tBest epoch:\", i)\n",
    "            print(\"\\tBest F-Score:\", bestScore[\"f1\"])\n",
    "            print(\"\\tHistory:\", [float(x[\"f1\"]) for x in checkpoint[\"scores_history\"]])\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "modelsInformationDF = pd.DataFrame(modelsInformation)\n",
    "\n",
    "printGPUStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsInformationDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageLimitedDataset import ImageLimitedDataset\n",
    "\n",
    "print(\"[üß† MODELS EVALUATION]\")\n",
    "\n",
    "modelsEvals = []\n",
    "\n",
    "for dataset in getSubDirs(datasetsDir):\n",
    "    print(\"\\n\" + \"-\" * 15)\n",
    "    print(\"[üóÉÔ∏è TEST DATASET] {}\".format(dataset))\n",
    "    \n",
    "    datasetDir = os.path.join(datasetsDir, dataset)\n",
    "    testDir = os.path.join(datasetDir, \"test\")\n",
    "\n",
    "    normalizationParams = normalizationVals[dataset]\n",
    "    dataTransform = transforms.Compose([\n",
    "        transforms.Resize(inputSize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            normalizationParams[\"test\"][0],\n",
    "            normalizationParams[\"test\"][1]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    testDataset = ImageLimitedDataset(testDir, transform=dataTransform, use_cache=True, check_images=False)\n",
    "\n",
    "    for cls in testDataset.classes:\n",
    "        cls_index = testDataset.class_to_idx[cls]\n",
    "        num_cls = np.count_nonzero(\n",
    "            np.array(testDataset.targets) == cls_index)\n",
    "        print(\"\\t[üßÆ # ELEMENTS] {}: {}\".format(cls, num_cls))\n",
    "\n",
    "    setSeed(SEED)\n",
    "    testDataLoader = DataLoader(testDataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(modelsDir, followlinks=True)):\n",
    "        for fname in sorted(fnames):\n",
    "            path = os.path.join(root, fname)\n",
    "            model = torch.load(path)\n",
    "\n",
    "            modelDataset = model[\"dataset\"] if \"dataset\" in model.keys() else root.split(os.sep)[1]\n",
    "\n",
    "            modelPercents = \"/\".join([str(x) for x in getClassPercents(model[\"dataset_sizes\"])])\n",
    "\n",
    "            print()\n",
    "            print(\"[üßÆ EVALUATING] {} - {} {}\".format(\n",
    "                modelDataset,\n",
    "                model[\"model_name\"],\n",
    "                modelPercents\n",
    "            ))\n",
    "\n",
    "            modelToTest = model[\"model\"]\n",
    "            modelToTest = modelToTest.to(device, non_blocking=True)\n",
    "\n",
    "            scores = evaluateModel(modelToTest, testDataLoader)\n",
    "\n",
    "            modelsEvals.append({\n",
    "                    \"dataset\": dataset,\n",
    "                    \"model\": model[\"model_name\"],\n",
    "                    \"model_dataset\": modelDataset,\n",
    "                    \"balancing\": modelPercents,\n",
    "                    \"acc\": scores[\"acc\"],\n",
    "                    \"precision\": scores[\"precision\"],\n",
    "                    \"recall\": scores[\"recall\"],\n",
    "                    \"f1\": scores[\"f1\"],\n",
    "                })\n",
    "            \n",
    "            print(\"\\tAcc: {:.4f}\".format(scores[\"acc\"]))\n",
    "            print(\"\\tPre: {:.4f}\".format(scores[\"precision\"]))\n",
    "            print(\"\\tRec: {:.4f}\".format(scores[\"recall\"]))\n",
    "            print(\"\\tF-Score: {:.4f}\".format(scores[\"f1\"]))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            printGPUStats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF = pd.DataFrame(modelsEvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEvalsDF.to_csv(\"modelsEvaluations.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
